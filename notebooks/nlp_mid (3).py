# -*- coding: utf-8 -*-
"""NLP MID.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n3BqBOg-pYMMPcOMYY9rBjc3704gaTd4

# Importing Libraries
"""

#importing all the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder

#The dataset is tab-separated and has no headers so I used seperator and header
df = pd.read_csv("/content/SMSSpamCollection",sep="\t",header=None,names=["label", "text"])
df

"""# Pre processing textual data"""

#counting the values of ham and spam in the file
df['label'].value_counts()

#function to clean text
def clean_text(text):
    text = text.lower()    #converting everything to lowercase
    text = re.sub(r'[^a-z\s]', '', text) # Removing numbers and special characters
    return text

df['clean_text'] = df['text'].apply(clean_text)
df

# Converting text into a table of word counts (Bag of Words)
vectorizer = CountVectorizer()
x_nb = vectorizer.fit_transform(df['clean_text'])
y_nb = df['label']
df

"""# Train test split"""

# Splitting the data into 80% for training and 20% for testing.
x_train_nb, x_test_nb, y_train_nb, y_test_nb = train_test_split(x_nb, y_nb, test_size=0.2, random_state=1234)

"""# Naive Bayes as a baseline model"""

#training the Naive Bayes model
Mnb = MultinomialNB()
Mnb.fit(x_train_nb , y_train_nb)

#making predictions using the model with the test data
pred_nb = Mnb.predict(x_test_nb)

"""# Evaluation Metrics"""

#calculating the accuracy and detailed metrics of the model
accuracy = accuracy_score(y_test_nb, pred_nb)
print("Accuracy:", accuracy)

print(classification_report(y_test_nb, pred_nb))
print(confusion_matrix(y_test_nb, pred_nb))

"""# Logistic regression"""

# using TF-IDF which gives more importance to meaningful words
tfidf = TfidfVectorizer()
x_tfidf = tfidf.fit_transform(df["clean_text"])
y = df["label"]

#splitting the data into train and test data for logistic regression
x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_tfidf, df['label'], test_size=0.2, random_state=1234)

#training the logistic regression model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(x_train_lr, y_train_lr)

#making predictions for the logistic regression model
pred_lr = lr_model.predict(x_test_lr)

#Prints the overall accuracy and detailed metrics for lr .

print("Accuracy:", accuracy_score(y_test_lr, pred_lr))
print("\nClassification Report:\n")
print(classification_report(y_test_lr, pred_lr))

#Visualizing model performance using a confusion matrix
cm_lr = confusion_matrix(y_test_lr, pred_lr)

plt.figure()
plt.imshow(cm_lr)
plt.title("Confusion Matrix - Logistic Regression (TF-IDF)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.xticks([0, 1], ["Ham", "Spam"])
plt.yticks([0, 1], ["Ham", "Spam"])

for i in range(cm_lr.shape[0]):
    for j in range(cm_lr.shape[1]):
        plt.text(j, i, cm_lr[i, j], ha="center", va="center")

plt.show()

"""#NEURAL NETWORK (deep learning model)"""

#Installing the Gensim library
!pip install gensim

# Neural Network tools
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

#Breaking sentences into lists of words (Tokens)
df['tokens'] = df['clean_text'].apply(lambda x: x.split())

#creating Word Embeddings
w2v_model = Word2Vec(
    sentences=df["tokens"],
    vector_size=100,
    window=5,
    min_count=2,
    workers=4
)

# Converting each message into a single vector by averaging its word vectors
def document_vector(tokens, model):

    #Getting vectors for words that exist in the Word2Vec model
    vectors = [model.wv[word] for word in tokens if word in model.wv]

    # Return a zero vector if no words are found
    if len(vectors) == 0:
        return np.zeros(model.vector_size)

    # Averaging all the word vectors together
    return np.mean(vectors, axis=0)

#Applyying the vectorization to all messages in the dataset
x_w2v = np.array([
    document_vector(tokens, w2v_model)
    for tokens in df["tokens"]
])

#Label encoding to turn ham into 0 and spam into 1.
le = LabelEncoder()
y_nn = le.fit_transform(df["label"])

#splitting the data into train and test data for neural network
x_train_nn, x_test_nn, y_train_nn, y_test_nn = train_test_split(x_w2v, y_nn, test_size=0.2, random_state=1234)

#Building a sequential neural network by stacking layers
model = Sequential([
    Dense(128, activation="relu", input_shape=(100,)),  #relu is the math formula that helps the model learn complex patterns.
    Dropout(0.5), # preventing the model from memorizing specific messages
    Dense(64, activation="relu"),
    Dropout(0.3),
    Dense(1, activation="sigmoid")  #This gives us a percentage
])

#Compiling the model with optimizer, loss function, and evaluation metric
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

#training the model
model.fit(
    x_train_nn, y_train_nn,
    epochs=10,
    batch_size=32,
    validation_split=0.1
)

#testing the model on the 20% it has never seen before.
pred_nn = model.predict(x_test_nn)
y_pred = (pred_nn > 0.5).astype(int) # If the probability is higher than 0.5, label it as spam


print("Accuracy:", accuracy_score(y_test_nn, y_pred))
print(classification_report(y_test_nn, y_pred, target_names=["ham", "spam"]))

# Creating and displaying a confusion matrix to evaluate the neural networkâ€™s predictions
cm = confusion_matrix(y_test_nn, y_pred)

plt.figure()
plt.imshow(cm)
plt.title("Confusion Matrix - Word2Vec + Neural Network")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.xticks([0, 1], ["Ham", "Spam"])
plt.yticks([0, 1], ["Ham", "Spam"])

for i in range(2):
    for j in range(2):
        plt.text(j, i, cm[i, j], ha="center", va="center")

plt.show()

"""#COMPARISON CHART"""

from sklearn.metrics import precision_recall_fscore_support

results = []

models_to_evaluate = [
    ('Naive Bayes', y_test_nb, pred_nb),
    ('Logistic Regression', y_test_lr, pred_lr),
    ('Neural Network', y_test_nn, y_pred)
]

for name, y_true, y_pred in models_to_evaluate:

    # Normalizing labels to binary format
    def normalize(labels):
        labels = np.array(labels).flatten()
        clean = []
        for x in labels:
            val = str(x).lower().strip()
            if val in ['spam', '1', '1.0']:
                clean.append(1)
            else:
                clean.append(0)
        return clean

    # Ensure true and predicted labels are consistent
    y_true_final = normalize(y_true)
    y_pred_final = normalize(y_pred)

    #Calculating accuracy
    acc = accuracy_score(y_true_final, y_pred_final)

    #Calculating evaluation metrics
    p, r, f1, _ = precision_recall_fscore_support(
        y_true_final,
        y_pred_final,
        average='binary',
        pos_label=1
    )

    #Save the results to list
    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': p,
        'Recall': r,
        'F1-Score': f1
    })

# Creating comparison table
comparison_df = pd.DataFrame(results)

print("\n--- PERFORMANCE ANALYSIS TABLE ---")
print(comparison_df.round(3))

# Creating the comparison bar chart
comparison_df.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title('Performance Comparison: Statistical vs. Deep Learning')
plt.ylabel('Score')
plt.ylim(0, 1.1)
plt.xticks(rotation=0)
plt.grid(axis='y', alpha=0.3)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

